> 目录

1\. 性能 1

[1.1. 服务器UDP丢包导致性能变差 1](\l)

[1.2. JDBC INSERT INTO VALUES很慢 2](\l)

[1.3. ORC表占用磁盘空间增长过快 2](\l)

[1.4. 某些SELECT语句hang在getaddrinfo调用上 2](\l)

[1.5. 增加节点后查询变慢 5](\l)

[1.6. connection get none response in 3600 seconds 5](\l)

[1.7. Hang一小时后报错 6](\l)

[1.8. psql报错 7](\l)

[2. 功能 7](\l)

[2.1. 如何以utility模式连接到数据库 7](\l)

[2.2. 如何以upgrade模式连接到数据库 7](\l)

[2.3. 僵死进程无法kill 7](\l)

[2.4. 设置完crontab信息后定时任务不生效 8](\l)

[2.5. 查询一个数据库下的所有hash表 8](\l)

[2.6. AO/Parquet表ALTER TABLE ADD COLUMN报错 8](\l)

[2.7. no partition for partitioning key 8](\l)

[2.8. invalid page header in block xxx of relation 8](\l)

[2.9. drop table很慢 9](\l)

[2.10. 显示时隐藏分区表的子分区表 9](\l)

[2.11. 启动数据库时core dump 12](\l)

[2.12. 跑批时进程挂死 13](\l)

[2.13. 分区特别多的表insert时过载报错 13](\l)

[2.14. kettle连接oushudb跑任务时报database connection closed 13](\l)

[2.15. pglog增长过快 14](\l)

[2.16. 自己写的python模块如何在PL/Python Function中import 14](\l)

[2.17. 运行union all时datanode挂掉 14](\l)

[2.18. pg\_log的rolling strategy 15](\l)

[2.19. 查看查询的历史记录 15](\l)

[2.20. 删除oushudb中的magma表信息 15](\l)

[2.21. 如何打开HDFS的local read 16](\l)

[2.22. 大规模使用分区表时查询性能变慢 16](\l)

[2.23. operation category READ is not supported in state standby 17](\l)

[2.24. 关掉master之后在standby上启动activate standby报错 18](\l)

[2.25. 关掉master之后在standby上启动activate standby报错 18](\l)

[2.26. 数据库长期运行后hcatalog xid无法回收 19](\l)

[2.27. analyzedb是否可以skip掉一些schema 19](\l)

[2.28. pg\_relation\_size是否会统计副本大小 19](\l)

[2.29. cannot open more than xxx append-only table segment files
concurrently 19](\l)

[2.30. 加ORC format的分区表的语法 19](\l)

[2.31. 如何使用三级及以上分区表 20](\l)

[2.32. 元数据bloat之后导致数据库性能变慢 20](\l)

[2.33. 如何查看数据库中索引和表的大小？ 21](\l)

[2.34. OushuDB中分区表的最佳实践 21](\l)

[2.35. Master和standby的手工同步 22](\l)

[2.36. gp\_segment\_configuration状态变化不及时，SQL查询报错也不发生变化
22](\l)

[2.37. interconnect诊断 22](\l)

[3. 资源管理 30](\l)

[3.1. insufficient memory reserved for statement 30](\l)

[3.2. too many open files 31](\l)

[3.3. analyze偶尔占用太长时间 32](\l)

[3.4. QE使用内存不到30%被oom kill 32](\l)

[4. Hadoop相关 32](\l)

[4.1. safe mode 32](\l)

[4.2. zookeeper listener port起不来 33](\l)

[4.3. Requested data length longer than max configured RPC length
33](\l)

[4.4. cannot fetch block locations 34](\l)

[4.5. 无法访问zookeeper服务 34](\l)

[4.6. 无法停止HDFS集群 35](\l)

[4.7. could not write in table 35](\l)

[4.8. time out waiting for a quorum of nodes to response 35](\l)

[4.9. flush failed for required journal 36](\l)

[4.10. too many clients already 41](\l)

[4.11. 在访问hive外部表的时候报错如下：ERROR: the pxf protocol for
external tables is deprecated，提示pxf协议已弃用 42](\l)

[4.12. 在使用hive的外部表进行读取数据的时候显示 ERROR: text\_in: failed
to get next tuple. File /hive/f00002\_ke\_e/000001\_0, offset is 0:
missing data for last 17 column(s), line is xxxxxx 42](\l)

[4.13. HDFS namenode全部挂掉，重启后oushudb查询个别表报错 42](\l)

[5. 安装部署 44](\l)

[5.1. hawq服务不能初始化或者启动 44](\l)

[5.2. 初始化时报错sync hawq-site.xml failed 45](\l)

[5.3. hawq start失败报错 45](\l)

[5.4. hawq init standy的问题 45](\l)

[5.5. hawq stop smart immediate fast的区别 46](\l)

[5.6. 修改hawq的ssh端口 46](\l)

[5.7. hawq init cluster时失败，提示输入密码 46](\l)

[5.8. hawq init cluster时失败，但已启成功magma service 46](\l)

[5.9. VMWare related issue 46](\l)

[6. ETL 47](\l)

[6.1. gpfdist报警告Address already in use 47](\l)

[6.2. 创建外部表报错create dir failed 47](\l)

[6.3. 创建外部表报错cannot create table on HDFS 47](\l)

[6.4. 访问外部表时报错failed to get block location of path 48](\l)

[6.5. 使用gpfdist时报错Possible SYN flooding 48](\l)

[6.6. 如何通过JDBC将spark和hawq连接起来 48](\l)

[6.7. data line too long 49](\l)

[7. 数据迁移 49](\l)

[7.1. 从db2迁移时对于substr的处理 49](\l)

[8. 操作系统 50](\l)

[8.1. ulimit -a通过ssh不生效 50](\l)

[8.2. linux关闭防火墙 50](\l)

[8.3. linux关闭selinux 50](\l)

[8.4. yum install时显示No module named site 50](\l)

[8.5. linux上进行磁盘挂载 51](\l)

[8.6. 磁盘分卷 51](\l)

[8.7. 如何检查操作系统版本和内核 54](\l)

[8.8. 如何更改主机名 54](\l)

[8.9. Java相关 54](\l)

[8.10. 如何YUM源配置 55](\l)

[8.11. scp指令快速指导 56](\l)

[8.12. sftp指令（可以用于Linux和Windows系统间的文件传输） 56](\l)

[8.13. 如何安装httpd 57](\l)

[9. 客户端 57](\l)

[9.1. pgAdminIII启动报错存储空间不足 57](\l)

[10. 升级 58](\l)

[10.1. 升级完成后部分任务运行报错，导致任务无法完成，出现connection
pointer is NULL && bucket num与实际文件不符两个错误 58](\l)

[10.2. 任务并发出现oom导致任务无法完成 59](\l)

[10.3. 有部分任务随机hang住 60](\l)

[10.4. 如何修改系统表找回丢失的列信息 60](\l)

[10.5. 如何在hawq升级过程中安装postgis 61](\l)

性能 {#性能 .35}
====

服务器UDP丢包导致性能变差 {#服务器udp丢包导致性能变差 .36}
-------------------------

-   问题

1)  全部SQL运行异常缓慢

2)  通过设置interconnect的报文发送重试超时为120 S（

> SET gp\_interconnect\_log\_stats=TRUE;
>
> SET
> gp\_interconnect\_transmit\_timeout=120;），然后运行有问题的查询，有如下超时报错：
>
> NOTICE: Interconnect encountered a network error, please check you
> network (seg19, 10.2.64.20:40000 pid=226133)
>
> DETAIL:
>
> Failed to send packet (seq 1) to 10.2.64.17:28332 (pid 788003 cid 3)
> after 124 retries in 120 seconds

3)  重复执行该查询，发现报错稳定，且总是向10.2.64.17发送报文重试失败。

-   分析

1)  全部SQL运行异常缓慢说明出问题的点不是某个表或者数据库中的某个实体对象出现故障。

2)  全部SQL运行异常的时间长度远超实际运行时长，说明不属于数据分布偏移或者资源欠缺等导致的性能下降，这类行为往往导致的性能下降一般在几倍的量级。

3)  通过设置超时到120S得到的报错证明，确实出现了网络UDP持续丢包现象导致数据传输无法顺利完成，数据库执行器在通讯中不断尝试重发结果失败。

4)  通过设置超时到120S得到的报错证明，10.2.64.17中稳定出现故障，应成为检查合排查的首要目标。

-   解答

1)  可尝试修改mtu（最大传输单元） 减小mtu值避免组包拆包
    但这个方案会影响到性能 也不一定能解决丢包的问题。

2)  如果方便协调，可尝试更换交换机口，将10.2.64.17那台机器换个口试试。

3)  如果方便，可尝试重启服务器或者重启网卡。

JDBC INSERT INTO VALUES很慢 {#jdbc-insert-into-values很慢 .36}
---------------------------

-   问题

> 通过JDBC执行大量的INSERT INTO VALUES语句时性能很慢

-   解答

> 采用COPY等方式将小量数据加载入库，用gpfdist外部表将大量数据加载入库。

ORC表占用磁盘空间增长过快 {#orc表占用磁盘空间增长过快 .36}
-------------------------

-   问题

    通过INSERT INTO
    VALUES语句频繁向ORC表插入数据后，磁盘空间使用量增长过快

-   解答

    在使用ORC或者Parquet表的情况下，避免采用INSERT INTO
    VALUES的方式插入数据，以避免造成数据膨胀。可以采用COPY等方式将小量数据加载入库，用gpfdist外部表将大量数据加载入库。

某些SELECT语句hang在getaddrinfo调用上 {#某些select语句hang在getaddrinfo调用上 .36}
-------------------------------------

-   问题

> 在某些版本的CentOS 7.x上执行如下查询时可能导致执行hang住：
>
> create TEMP table tt (a int, b float4, c text);
>
> insert into tt SELECT g, g, NULL from generate\_series(1, 10000000)g;
>
> SELECT count(\*) from tt where tt.c is null;
>
> 其错误的栈类似下面：
>
> Thread 2 (Thread 0x7f1f03b26700 (LWP 4712)):
>
> \#0 0x00007f1eff58a69d in poll () from /lib64/libc.so.6
>
> \#1 0x0000000000bbf874 in rxThreadFunc (arg=\<optimized out\>) at
> ic\_udpifc.c:6277
>
> \#2 0x00007f1f0008bdc5 in start\_thread () from /lib64/libpthread.so.0
>
> \#3 0x00007f1eff594ced in clone () from /lib64/libc.so.6
>
> Thread 1 (Thread 0x7f1f03bd8740 (LWP 4711)):
>
> \#0 0x00007f1eff595c5d in recvmsg () from /lib64/libc.so.6
>
> \#1 0x00007f1eff5b84cd in make\_request () from /lib64/libc.so.6
>
> \#2 0x00007f1eff5b89c4 in \_\_check\_pf () from /lib64/libc.so.6
>
> \#3 0x00007f1eff57ea89 in getaddrinfo () from /lib64/libc.so.6
>
> \#4 0x0000000000bb9c81 in setupUDPListeningSocket
> (listenerSocketFd=0x2599a34, listenerPort=0x7fffcffb270c,
> txFamily=\<optimized out\>) at ic\_udpifc.c:1231
>
> \#5 0x0000000000bbe8dd in startOutgoingUDPConnections
> (pOutgoingCount=\<optimized out\>, sendSlice=\<optimized out\>,
> transportStates=\<optimized out\>) at ic\_udpifc.c:2987
>
> \#6 SetupUDPIFCInterconnect\_Internal (estate=\<optimized out\>) at
> ic\_udpifc.c:3460
>
> \#7 SetupUDPIFCInterconnect (estate=\<optimized out\>) at
> ic\_udpifc.c:3521
>
> \#8 0x00000000007548fa in ExecutorStart (queryDesc=\<optimized out\>,
> eflags=\<optimized out\>) at execMain.c:517
>
> \#9 0x000000000099de15 in ProcessQuery (portal=\<optimized out\>,
> stmt=0x251a300, params=\<optimized out\>, dest=\<optimized out\>,
> completionTag=\<optimized out\>) at pquery.c:282
>
> \#10 0x000000000099ed19 in PortalRunMulti (portal=0x252c520,
> isTopLevel=1 \'\\001\', dest=\<optimized out\>, altdest=\<optimized
> out\>, completionTag=0x7fffcffb2c90 \"\") at pquery.c:1603
>
> \#11 0x00000000009a0515 in PortalRun (portal=\<optimized out\>,
> count=\<optimized out\>, isTopLevel=0 \'\\000\', dest=\<optimized
> out\>, altdest=\<optimized out\>, completionTag=\<optimized out\>) at
> pquery.c:1125
>
> \#12 0x00000000009994f3 in exec\_mpp\_query (localSlice=\<optimized
> out\>, seqServerPort=\<optimized out\>, seqServerHost=\<optimized
> out\>, serializedSliceInfolen=\<optimized out\>,
> serializedSliceInfo=\<optimized out\>, serializedParamslen=\<optimized
> out\>, serializedParams=\<optimized out\>,
> serializedPlantreelen=\<optimized out\>,
> serializedPlantree=\<optimized out\>,
> serializedQuerytreelen=\<optimized out\>,
> serializedQuerytree=\<optimized out\>, query\_string=\<optimized
> out\>) at postgres.c:1358
>
> \#13 PostgresMain (argc=\<optimized out\>, argv=\<optimized out\>,
> dbname=0x2390788 \"dsrprd\", username=\<optimized out\>) at
> postgres.c:4905
>
> \#14 0x00000000008f7eae in BackendRun (port=\<optimized out\>) at
> postmaster.c:6963
>
> \#15 BackendStartup (port=\<optimized out\>) at postmaster.c:6658
>
> \#16 ServerLoop () at postmaster.c:2464
>
> \#17 0x00000000008fac30 in PostmasterMain (argc=15, argv=0x234b4a0) at
> postmaster.c:1540
>
> \#18 0x00000000007fccaf in main (argc=15, argv=0x234b430) at
> main.c:206

-   解答

> 这是操作系统的bug，需要升级系统内核。具体见：
>
> https://access.redhat.com/support/cases/\#/case/01596655
>
> [[https://access.redhat.com/solutions/2379961]{.underline}](https://access.redhat.com/solutions/2379961)

增加节点后查询变慢 {#增加节点后查询变慢 .36}
------------------

-   解答

> 查看data locality是否小于1，需要做一次rebalance

connection get none response in 3600 seconds {#connection-get-none-response-in-3600-seconds .36}
--------------------------------------------

-   问题

> 运行之前可以执行成功的sql查询语句报错，报错信息"connection xxx get
> none response from xxx in 3600 seconds. It may \..."。

-   解答

> 这是因为节点间有UDP丢包。
>
> UDP Packet Lost - receive buffer errors。
>
> 确认是否是packet reassembles failed 导致UDP丢包
>
> \#执行命令
>
> \$ cat /proc/net/snmp \| grep \"Ip\"
>
> Ip: Forwarding DefaultTTL InReceives InHdrErrors InAddrErrors
> ForwDatagrams InUnknownProtos InDiscards InDelivers OutRequests
> OutDiscards OutNoRoutes ReasmTimeout ReasmReqds ReasmOKs ReasmFails
> FragOKs FragFails FragCreates
>
> Ip: 1 64 148635009 0 0 0 0 0 148628477 148758559 0 0 0 0 0 0 0 0 0
>
> \#确认 ReasmTimeout 与 ReasmFails 值
>
> \- ReasmTimeout: 分片在reassemble buffer中超时后被丢弃的计数
>
> \- ReasmFails: 报文重组失败计数。该值包含timeout的计数
>
> 确认方法
>
> 1.
> 当系统中有大量的ReasmTimeout计数的增长时，一般需要确认的是底层链路是否存在一定的丢包率
>
> 2\. packet reassembles failed 计数一直在增长
>
> 3\. 通过ReasmTimeout的计数排除掉是timeout
> 造成的failed增长，或者timeout的计数只是failed中的一部分
>
> 4\. 通过ReasmFails
> 增长的规律进行判断，因为buffer满后丢包的过程是会有大量丢包的，所以这个时候
> packet reassembles failed 是一次性巨量的增长，而非1、2的增加。
>
> 可以将gp\_max\_packet\_size设置为小于MTU
> (1500)的值，比如1024，然后重启集群。

Hang一小时后报错 {#hang一小时后报错 .36}
----------------

-   问题

> hang 1小时后报错
>
> analyze 相比之前慢一个小时
>
> 残留qe的栈hang在receiveChunksUDP SendChunkUDP SendEosUDP

-   解答

1.  通过 hawq ssh 执行 ps -ef \| grep postgres 查看当时qe的残留情况

2.  通过残留qe进程号查找segment的hawq日志中所有相关日志，重点关注cancel和error字眼，若是当时qe已经不在，可以搜索"ailed
    > to send packet"

3.  当有qe 栈hang在SendChunkUDP或SendEosUDP 可以找是否有qe
    > hang在receiveChunksUDP，同理若是有qe栈hang在
    > SendEosUDP或者SendChunkUDP。

4.  若排查udp丢包问题，先通过 netstat -anup \| grep
    > qe的进程号，获取ic-udp的端口号， 在执行 tcpdump -i 网卡 port
    > 端口号 查看收发的报文， 若观察到hang在发送端的机器有发包，
    > 接收端机器收不到包，就说明udp报文丢了。

psql报错 {#psql报错 .36}
--------

-   问题

> psql报错 connect失败等类型的内容

-   解答

> 若sql plan的slice数比较多或者并发比较高需要调大 SegMaxBackends
> 相应的值要大于 并发数 \* slice数 \* vseg数， 但要小于10000，
> 否则会启动失败。 也要更改操作系统参数 net.core.somaxconn，同样要求大于
> 并发数 \* slice数 \* vseg数

功能 {#功能 .35}
====

如何以utility模式连接到数据库 {#如何以utility模式连接到数据库 .36}
-----------------------------

-   解答

> PGOPTIONS=\'-c gp\_session\_role=utility\'
>
> psql \[-p PORT\] \<database\>

如何以upgrade模式连接到数据库 {#如何以upgrade模式连接到数据库 .36}
-----------------------------

-   解答

> upgrade\_mode=true
>
> allow\_system\_table\_mods=ddl/dml
>
> PGOPTIONS=\'-c gp\_session\_role=utility\'
>
> psql \[-p PORT\] \<database\>

僵死进程无法kill  {#僵死进程无法kill .36}
-----------------

-   问题

> 进程在读写磁盘时僵死，导致对应的应用程序没有响应，利用kill也无法清除该进程

-   解答

4)  重启操作系统可以临时缓解改问题

5)  检查磁盘是否有损坏或者操作系统有bug

设置完crontab信息后定时任务不生效 {#设置完crontab信息后定时任务不生效 .36}
---------------------------------

-   分析

> 使用相对路径设置了可执行程序

-   解答

> cd \$PATH\_TO\_EXECUTABLE; ./EXECUTABLE \$PARAMETERS
>
> \$PATH\_TO\_EXECUTABLE/EXECUTABLE \$PARAMETERS

查询一个数据库下的所有hash表 {#查询一个数据库下的所有hash表 .36}
----------------------------

-   解答

> SELECT relname FROM pg\_class pc, gp\_distribution\_policy gdp WHERE
> pc.oid = gdp.localoid AND gdp.attrnums IS NOT NULL;

AO/Parquet表ALTER TABLE ADD COLUMN报错 {#aoparquet表alter-table-add-column报错 .36}
--------------------------------------

-   解答

> AO/Parquet表ALTER TABLE ADD COLUMN时必须指定default value

no partition for partitioning key {#no-partition-for-partitioning-key .36}
---------------------------------

-   问题

> 往分区表插入数据时报错：no partition for partitioning key

-   解答

> 检查插入的数据是否有对应的分区

invalid page header in block xxx of relation {#invalid-page-header-in-block-xxx-of-relation .36}
--------------------------------------------

-   问题

> 数据有坏块，查询报错 \"invalid page header in block xxx of relation\"

-   解答

> 清理掉坏块：
>
> SET zero\_damaged\_pages = on;
>
> VACUUM FULL tablename;
>
> reindex table tablename;

drop table很慢 {#drop-table很慢 .36}
--------------

-   解答

> 建议使用vacuum & reindex系统表避免元数据膨胀。

显示时隐藏分区表的子分区表 {#显示时隐藏分区表的子分区表 .36}
--------------------------

-   问题

> 分区表太多在DBvisual或者PGAdmin里面看起来太长，找不到表。希望我们能在这里隐藏分区表。

-   解答

> 可以先创建新的用于存储parittion
> table的schema，然后在新的schema内创建partition
> table，最后在默认的schema内创建一个同名的view指向partition table。
>
> 这样的话只会在存储parittion table的schema内看见partition
> table的子表，但是在默认的schema内只看见与partition
> table同名的view。该view的功能和原有的partition table是一致的。
>
> demo=\# show search\_path ;
>
> search\_path
>
> \-\-\-\-\-\-\-\-\-\-\-\-\-\-\--
>
> \"\$user\",public
>
> (1 row)
>
> demo=\# \\d
>
> No relations found.
>
> demo=\# create schema sales;
>
> CREATE SCHEMA
>
> demo=\# CREATE TABLE sales.sales (id int, date date, amt
> decimal(10,2)) DISTRIBUTED BY (id) PARTITION BY RANGE (date) ( START
> (date \'2008-01-01\') INCLUSIVE END (date \'2008-01-10\') EXCLUSIVE
> EVERY (INTERVAL \'1 day\') );
>
> NOTICE: CREATE TABLE will create partition \"sales\_1\_prt\_1\" for
> table \"sales\"
>
> NOTICE: CREATE TABLE will create partition \"sales\_1\_prt\_2\" for
> table \"sales\"
>
> NOTICE: CREATE TABLE will create partition \"sales\_1\_prt\_3\" for
> table \"sales\"
>
> NOTICE: CREATE TABLE will create partition \"sales\_1\_prt\_4\" for
> table \"sales\"
>
> NOTICE: CREATE TABLE will create partition \"sales\_1\_prt\_5\" for
> table \"sales\"
>
> NOTICE: CREATE TABLE will create partition \"sales\_1\_prt\_6\" for
> table \"sales\"
>
> NOTICE: CREATE TABLE will create partition \"sales\_1\_prt\_7\" for
> table \"sales\"
>
> NOTICE: CREATE TABLE will create partition \"sales\_1\_prt\_8\" for
> table \"sales\"
>
> NOTICE: CREATE TABLE will create partition \"sales\_1\_prt\_9\" for
> table \"sales\"
>
> CREATE TABLE
>
> demo=\# create view sales as select \* from sales.sales;
>
> CREATE VIEW
>
> demo=\# \\d
>
> List of relations
>
> Schema \| Name \| Type \| Owner \| Storage
>
> \-\-\-\-\-\-\--+\-\-\-\-\-\--+\-\-\-\-\--+\-\-\-\-\-\--+\-\-\-\-\-\-\-\--
>
> public \| sales \| view \| admin \| none
>
> (1 row)
>
> demo=\# \\d sales
>
> View \"public.sales\"
>
> Column \| Type \| Modifiers
>
> \-\-\-\-\-\-\--+\-\-\-\-\-\-\-\-\-\-\-\-\-\--+\-\-\-\-\-\-\-\-\-\--
>
> id \| integer \|
>
> date \| date \|
>
> amt \| numeric(10,2) \|
>
> View definition:
>
> SELECT sales.id, sales.date, sales.amt
>
> FROM sales.sales;
>
> demo=\# set search\_path to sales;
>
> SET
>
> demo=\# \\d
>
> List of relations
>
> Schema \| Name \| Type \| Owner \| Storage
>
> \-\-\-\-\-\-\--+\-\-\-\-\-\-\-\-\-\-\-\-\-\--+\-\-\-\-\-\--+\-\-\-\-\-\--+\-\-\-\-\-\-\-\-\-\-\-\--
>
> sales \| sales \| table \| admin \| append only
>
> sales \| sales\_1\_prt\_1 \| table \| admin \| append only
>
> sales \| sales\_1\_prt\_2 \| table \| admin \| append only
>
> sales \| sales\_1\_prt\_3 \| table \| admin \| append only
>
> sales \| sales\_1\_prt\_4 \| table \| admin \| append only
>
> sales \| sales\_1\_prt\_5 \| table \| admin \| append only
>
> sales \| sales\_1\_prt\_6 \| table \| admin \| append only
>
> sales \| sales\_1\_prt\_7 \| table \| admin \| append only
>
> sales \| sales\_1\_prt\_8 \| table \| admin \| append only
>
> sales \| sales\_1\_prt\_9 \| table \| admin \| append only
>
> (10 rows)

启动数据库时core dump {#启动数据库时core-dump .36}
---------------------

-   问题

> 2018-10-17 16:47:22.396312
> CST,,,p9255,th164806080,,,,0,,,seg-10000,,,,,\"LOG\",\"00000\",\"3rd
> party error log:
>
> error : Opening and ending tag mismatch: property line 168 and
> configuration
>
> \</configuration\>
>
> \^\",,,,,,,,\"SysLoggerMain\",\"syslogger.c\",518,
>
> 2018-10-17 16:47:22.412744
> CST,,,p9254,th164806080,,,,0,,,seg-10000,,,,,\"FATAL\",\"XX000\",\"Fail
> to parse file /usr/local/hawq/./etc/hawq-site.xml for reading
> configuration. (hawqsite.c:52)\",,,,,,,0,,\"hawqsite.c\",52,\"Stack
> trace:
>
> 1 0x810467 postgres errstart (??:?)
>
> 2 0x813433 postgres elog\_finish (??:?)
>
> 3 0x88e553 postgres getHawqSiteConfigurationList (??:?)
>
> 4 0x829a70 postgres ProcessConfigFile (??:?)
>
> 5 0x7174a3 postgres \<symbol not found\> (??:?)
>
> 6 0x7fd6068ab370 libpthread.so.0 \<symbol not found\> (??:0)
>
> 7 0x7fd60610db83 libc.so.6 \_\_select (??:0)
>
> 8 0x718145 postgres \<symbol not found\> (??:?)
>
> 9 0x7173e5 postgres PostmasterMain (??:?)
>
> 10 0x68b249 postgres main (??:?)
>
> 11 0x7fd606040b35 libc.so.6 \_\_libc\_start\_main (??:0)
>
> 12 0x4a2ffd postgres \<symbol not found\> (??:?)

-   解答

> 查看配置文件有人将HDFS参数加入到hawq-site.xml里，由于参数不对，后面在启动命令时就会报错。去掉HDFS参数后就可以正常启动。

跑批时进程挂死 {#跑批时进程挂死 .36}
--------------

-   问题

> 跑批时经常有进程无故挂死，有insert进程，自定义update函数等，导致数据库大量进程不能结束，锁住后来的进程。

-   解答

> 经过分析，发现insert时在analyze阶段会hang住，设置gp\_autostats\_mode为none时insert不会hang,但是之后手工analyze也有hang住情况，之前在其他客户poc也有类似情况发生。暂时gp\_autostats\_mode为none，analyze手工完成，有问题就停掉。

分区特别多的表insert时过载报错 {#分区特别多的表insert时过载报错 .36}
------------------------------

-   解答

> HDFS在加载时打开过多文件过载，INSERT INTO XXX SELECT \* FROM YYY WHERE
> partition-condition过滤掉部分partition数据，也就是说一批一批partition加载，防止hdfs在加载时打开过多文件过载。

kettle连接oushudb跑任务时报database connection closed {#kettle连接oushudb跑任务时报database-connection-closed .36}
-----------------------------------------------------

-   问题

> 用kettle连接hawq跑任务，然后出现hawq没有返回，导致后续任务无法继续执行。查看日志，在里面有个database
> connection closed，但是前面又说statement已经执行了。

-   解答

> 通过分析kettle的日志文件和oushudb的日志文件，sql都执行成功了。没有继续执行的那条query执行时间较长，但是执行成功了。可能是kettle没有正确分析出成功执行的日志。猜测是kettle的问题（kettle有设置超时参数1000s,在1000s以内的hawq
> query就基本上不报错）。

pglog增长过快 {#pglog增长过快 .36}
-------------

-   问题

> 发现pglog增长过快，oushudb能不写日志吗？或者这个路径能改吗？

-   解答

> pglog的路径是自动生成的，如果想要减少日志，可以提供多种方案。可以修改，但是需要重新初始化HAWQ；如果不需要分析日志，可以直接通过修改日志输出级别来过滤掉大部分日志；如果需要日志，可以通过mount磁盘到该节点，或者
> 将原有的hawq 日志路径link到其他地方。

自己写的python模块如何在PL/Python Function中import {#自己写的python模块如何在plpython-function中import .36}
--------------------------------------------------

-   解答

> 参考[[http://hawq.apache.org/docs/userguide/2.3.0.0-incubating/plext/using\_plpython.html\#enableplpython]{.underline}](http://hawq.apache.org/docs/userguide/2.3.0.0-incubating/plext/using_plpython.html#enableplpython)

运行union all时datanode挂掉 {#运行union-all时datanode挂掉 .36}
---------------------------

-   问题

> 运行union all的查询语句时，时常出现HDFS挂掉的情况
>
> 检查后发现hawq还在，查看hdfs的datanode都已经不在了，查看hdfs的日志限制内存分配失败。
> 用户机器的内存为64g， hawq-site.xml中配置成48g,
> 但是操作系统的参数overcommit\_ratio配的是50，导致hawq减少内存使用并没有提高hdfs的内存分配额度。

-   解答

> 将/proc/sys/vm/overcommit\_ratio的操作系统参数由原来的50改成100

pg\_log的rolling strategy {#pg_log的rolling-strategy .36}
-------------------------

-   问题

> pg\_log增长较快，能否设置为每1G一个log文件。

-   解答

> 默认情况下，每次重启或者下一天时会创建一个新的log文件，也可以设置log\_rotation\_size
> （单位为KB，默认值为0，表示关闭此功能）来限制每个log文件的大小。

查看查询的历史记录 {#查看查询的历史记录 .36}
------------------

-   问题

> 如何查看数据库里处理过的查询的历史记录？

-   解答

> SET enable\_pg\_stat\_activity\_history = TRUE;
>
> SELECT \* FROM hawq\_toolkit.\_\_hawq\_pg\_stat\_activity\_history;

删除oushudb中的magma表信息 {#删除oushudb中的magma表信息 .36}
--------------------------

-   问题

> 在magma端初始化后，如何删除hawq
> catalog中的magma表信息而不重新初始化hawq？

-   解答

> \$ cat delete\_catalog.sh
>
> export PGOPTIONS=\'-c gp\_session\_role=utility\'
>
> database=\$1
>
> schema=\$2
>
> tablename=\$3
>
> oid=\`psql -Atc \"select oid from pg\_class where
> relname=lower(\'\${tablename}\') and exists (select 1 from
> pg\_namespace where nspname=\'\${schema}\' and
> oid=pg\_class.relnamespace);\" -d \$database\`
>
> echo \"
>
> begin;
>
> set allow\_system\_table\_mods=\'dml\';
>
> delete from gp\_distribution\_policy where localoid=\'\${oid}\';
>
> delete from pg\_type where typrelid=\'\${oid}\';
>
> delete from pg\_depend where refobjid=\'\${oid}\';
>
> delete from pg\_attribute where attrelid=\'\${oid}\';
>
> delete from pg\_exttable where reloid=\'\${oid}\';
>
> delete from pg\_class where oid=\'\${oid}\';
>
> commit;
>
> \" \|psql -d postgres

如何打开HDFS的local read {#如何打开hdfs的local-read .36}
------------------------

-   解答

> 1） HDFS server
>
> \$ cat \$HADOOP\_HOME/etc/hadoop/hdfs-site.xml
>
> \<property\>
>
> \<name\>dfs.domain.socket.path\</name\>
>
> \<value\>/var/lib/hadoop-hdfs/dn\_socket\</value\>
>
> \</property\>
>
> 2） HDFS client
>
> \$ cat \$GPHOME/etc/hdfs-client.xml
>
> \<property\>
>
> \<name\>dfs.domain.socket.path\</name\>
>
> \<value\>/var/lib/hadoop-hdfs/dn\_socket\</value\>
>
> \</property\>
>
> 上面1），2）中的路径一致，且socket文件的权限为666，即hawq用户可写。

大规模使用分区表时查询性能变慢 {#大规模使用分区表时查询性能变慢 .36}
------------------------------

-   问题

> 使用大规模分区表的时候有时会出现执行时间不合理长的问题，调研log发现是计划阶段，会有大量访问元数据表的操作，该操作占用文件句柄，然后检查了文件句柄缓冲，没发现特别问题，检查log看到有问题时会有下述提示，且大的时间间隔发生在这类log之间。另外从explain
> analyze发现，查询开始的start offset很大。

-   解答

> 检查发现master有自己bufmgr作为内部缓冲区（默认4096个）支持元数据表访问，当元数据多，访问负载高时，发生bufmgr的刷新切换。Mac本地在跑feature
> test负载 session较多情况下会出现类似提示，但没发现时间超长的问题。
>
> "writing block \<id\> of relation \<id\>/\<id\>/\<id\>
>
> 方案：暂时我们内部确定的可能调试方法是用guc
> 调大这个内部缓冲区可能会缓解元数据访问问题，但这个是使用共享内存的，不能任意调大。已建议用户降低分区表数量，建议128以内。
>
> {
>
> {\"shared\_buffers\", PGC\_POSTMASTER, RESOURCES\_MEM,
>
> gettext\_noop(\"Sets the number of shared memory buffers used by the
> server.\"),
>
> NULL,
>
> GUC\_UNIT\_BLOCKS
>
> },
>
> &NBuffers,
>
> 4096, 16, INT\_MAX / 2, NULL, NULL
>
> }

operation category READ is not supported in state standby {#operation-category-read-is-not-supported-in-state-standby .36}
---------------------------------------------------------

-   解答

> 生产环境，Oushu
> DB使用甲方自己运行的Hadoop作为存储，Hadoop集群扩容客户自行重启后namenode
> active节点切换到了另一个原standby的节点上，导致sql运行报错：
>
> ERROR: cannot fetch block locations
>
> Operation category READ is not supported in state standby
>
> 检查发现OushuDB的filespace
> entry初始化的时候没有配置成servicename的方式：客户环境监测结果如下：
>
> select \* from pg\_filespace\_entry;
>
> hdfs://bigdata-m-005:8020/hawq\_data
>
> 期待的应该是：
>
> hdfs://ns1/hawq\_data
>
> 方案：通过使用move filespace的方法修改数据库。操作命令：
>
> 1）hawq stop cluster -a -M fast （需要用停止的库去更新）
>
> 2）备份masterdd的数据，其中pg\_log部分可删除旧文件
>
> 3）export GPPORT (如果没有使用默认端口的话)
>
> 4）hawq filespace \--movefilespace \<filespace\>
> \--location=\<location\>
>
> 5）hawq start cluster -a (重启数据库)
>
> 6）重建standby
>
> 7）如果第四步失败，用备份元数据覆盖恢复

关掉master之后在standby上启动activate standby报错 {#关掉master之后在standby上启动activate-standby报错 .36}
-------------------------------------------------

-   解答

> hawq activate standby -i忽略掉被停掉的机器

关掉master之后在standby上启动activate standby报错 {#关掉master之后在standby上启动activate-standby报错-1 .36}
-------------------------------------------------

-   问题

> 数据有坏块时，查询报错 \"invalid page header in block xxx of
> relation\"

-   解答

> SET zero\_damaged\_pages = on;
>
> VACUUM FULL tablename;
>
> reindex table tablename;
>
> 可清理掉坏块。

数据库长期运行后hcatalog xid无法回收 {#数据库长期运行后hcatalog-xid无法回收 .36}
------------------------------------

-   问题

> 数据有坏块时，查询报错 \"invalid page header in block xxx of
> relation\"

-   解答

> hcatalog是external db，无法连上做vacuum freeze,
> 通过删除该数据库避开此问题。
>
> PGOPTIONS=\'-c gp\_session\_role=utility\' psql -p 40000 template1 -c
> \"set allow\_system\_table\_mods=\'DML\';delete from pg\_database
> where datname=\'hcatalog\';\"
>
> hawq restart segment -a
>
> PGOPTIONS=\'-c gp\_session\_role=utility\' psql -p 40000 template1 -c
> \'vacuum freeze\'

analyzedb是否可以skip掉一些schema {#analyzedb是否可以skip掉一些schema .36}
---------------------------------

-   解答

> 无法skip。但是可以通过-s选项来指定想要analyze的schema。比如analyzedb
> --s pg\_catalog --d postgres

pg\_relation\_size是否会统计副本大小 {#pg_relation_size是否会统计副本大小 .36}
------------------------------------

-   解答

> 不会。它只会统计原表大小，而不会统计副本大小。

cannot open more than xxx append-only table segment files concurrently {#cannot-open-more-than-xxx-append-only-table-segment-files-concurrently .36}
----------------------------------------------------------------------

-   解答

> max\_appendonly\_segfiles的默认值为262144，可以适当调大该值。

加ORC format的分区表的语法 {#加orc-format的分区表的语法 .36}
--------------------------

-   解答

> ALTER TABLE lineitem ADD DEFAULT PARTITION other with (format 'orc')。

如何使用三级及以上分区表 {#如何使用三级及以上分区表 .36}
------------------------

-   解答

> [[https://gp-docs-cn.github.io/docs/admin\_guide/ddl/ddl-partition.html\#topic70]{.underline}](https://gp-docs-cn.github.io/docs/admin_guide/ddl/ddl-partition.html#topic70)

元数据bloat之后导致数据库性能变慢 {#元数据bloat之后导致数据库性能变慢 .36}
---------------------------------

-   解答

1)  查看catalog表是否有bloat问题

> SET gp\_select\_invisible TO false;
>
> SELECT count(\*) FROM pg\_attribute; // n1
>
> SET gp\_select\_invisible TO true;
>
> SELECT count(\*) FROM pg\_attribute; // n2
>
> 设r =.n2 / n1，如果r \< 4，认为没有bloat；如果4 \<= r \<=
> 10,认为是中等bloat；如果r \> 10，认为是严重的bloat。

2)  解决catalog的bloat问题

> VACUUM ANALYZE pg\_attribute;
>
> REINDEX TABLE pg\_attribute;

3)  分析

> OushuDB
> master使用多版本机制来实现更新操作，所以会产生过时的版本。过时的版本会带来性能问题，需要针对每一个系统表运行VACUUM命令。
>
> 可以使用如下脚本来对一个数据库的所有元数据表做VACUUM和REINDEX。如果用户已经定期做VACUUM，REINDEX则不需要，可以从下面脚本中注释掉。
> REINDEX只有在长期没有做VACUUM后需要。
>
> \#!/bin/bash

VACUUM\_CMD=\"VACUUM **ANALYZE**\"

REINDEX\_CMD=\"REINDEX TABLE\"

if \[ \$\# != 1 \]; then

echo \"USAGE: vacuum.sh database-name\"

exit

fi

echo -e \"\\nBegin: vacuum and reindex database: \" \$1 \"\\n\"

psql -d \$1 -tc \"select \'\$VACUUM\_CMD\' \|\| \' pg\_catalog.\' \|\|
relname \|\| \';\' from pg\_class a,pg\_namespace b where
a.relnamespace=b.oid and b.nspname= \'pg\_catalog\' and
a.relkind=\'r\'\" \| psql -a \$1

psql -d \$1 -tc \"select \'\$REINDEX\_CMD\' \|\| \' pg\_catalog.\' \|\|
relname \|\| \';\' from pg\_class a,pg\_namespace b where
a.relnamespace=b.oid and b.nspname= \'pg\_catalog\' and
a.relkind=\'r\'\" \| psql -a \$1

> echo -e \"\\nNOTE: Please ignore the errors about some tables which
> cannot be reindexed.\\n\"

如何查看数据库中索引和表的大小？ {#如何查看数据库中索引和表的大小 .36}
--------------------------------

-   解答

> SELECT indexrelid::regclass idx\_name,
>
> indrelid::regclass tab\_name,
>
> pg\_size\_pretty(pg\_relation\_size(indexrelid)) idx\_size,
>
> pg\_size\_pretty(pg\_relation\_size(indrelid)) tab\_size,
>
> (pg\_relation\_size(indexrelid) - pg\_relation\_size(indrelid))
> size\_diff
>
> FROM pg\_index
>
> WHERE indrelid IN (SELECT oid FROM pg\_class where relnamespace=11)
>
> ORDER BY 5 DESC;

OushuDB中分区表的最佳实践 {#oushudb中分区表的最佳实践 .36}
-------------------------

-   解答

> 现在分区表仅剩下的一个缺点就是如果同时往很多个分区子表中插入数据的话会使用过多内存。不过一般情况下数据都是集中在一个分区。如果需要同时往很多子分区插入数据（比如初始数据加载），可以在INSERT
> INTO SELECT语句中加一个分区字段的ORDER
> BY，这样就会一个子分区一个子分区打开插入，并关闭，节省内存。

Master和standby的手工同步 {#master和standby的手工同步 .36}
-------------------------

-   解答

> source /usr/local/hawq/greenplum\_path.sh
>
> SELECT pg\_terminate\_backend(procpid), \* FROM pg\_stat\_activity
> WHERE procpid \<\> pg\_backend\_pid();
>
> hawq init standby -n
>
> SELECT \* FROM gp\_master\_mirroring;

gp\_segment\_configuration状态变化不及时，SQL查询报错也不发生变化 {#gp_segment_configuration状态变化不及时sql查询报错也不发生变化 .36}
-----------------------------------------------------------------

-   解答

> 设置了guc hawq\_rm\_enable\_connpool
> 为false就可以跑一条SQL就立马检测出来了

interconnect诊断 {#interconnect诊断 .36}
----------------

-   问题

> SQL 执行挂起，进程hang在数据发送或者接受中

-   分析&&解答

1.  **检查进程状态确认是否是相关问题**

> 有时候会遇到QD运行不退出，挂住的情况，此时在上边看到除了QD进程外还有至少一个QE进程处于未退出状态，通过使用hawq
> ssh
> 配合ps命令可进行检查，观察QD的connection号码，找到和其配套的QE进程都有哪些从而避免并行SQL的干扰。比如下面就是客户现场的具体进程状态，供参考：

\[bigdata-w-011\] gpadmin 202850 599486 0 15:30 ? 00:00:02 postgres:
port 40000, llys llysdb 10.223.6.8(55393) con83087 seg6 cmd10725 slice21
MPPEXEC SELECT

\[bigdata-w-011\] gpadmin 202852 599486 0 15:30 ? 00:00:01 postgres:
port 40000, llys llysdb 10.223.6.8(55395) con83087 seg8 idle

\[bigdata-w-011\] gpadmin 202855 599486 0 15:30 ? 00:00:01 postgres:
port 40000, llys llysdb 10.223.6.8(55407) con83087 seg6 idle

\[bigdata-w-011\] gpadmin 202862 599486 0 15:30 ? 00:00:03 postgres:
port 40000, llys llysdb 10.223.6.8(55429) con83087 seg7 idle

\[bigdata-w-011\] gpadmin 202863 599486 0 15:30 ? 00:00:03 postgres:
port 40000, llys llysdb 10.223.6.8(55435) con83087 seg7 cmd10725 slice21
MPPEXEC SELECT

\[bigdata-w-011\] gpadmin 202864 599486 0 15:30 ? 00:00:01 postgres:
port 40000, llys llysdb 10.223.6.8(55451) con83087 seg6 idle

\[bigdata-w-011\] gpadmin 202874 599486 0 15:30 ? 00:00:01 postgres:
port 40000, llys llysdb 10.223.6.8(55469) con83087 seg7 idle

\[bigdata-w-011\] gpadmin 202875 599486 0 15:30 ? 00:00:01 postgres:
port 40000, llys llysdb 10.223.6.8(55473) con83087 seg9 idle

\[bigdata-w-011\] gpadmin 202877 599486 0 15:30 ? 00:00:02 postgres:
port 40000, llys llysdb 10.223.6.8(55477) con83087 seg8 cmd10725 slice21
MPPEXEC SELECT

\[bigdata-w-011\] gpadmin 202881 599486 0 15:30 ? 00:00:01 postgres:
port 40000, llys llysdb 10.223.6.8(55481) con83087 seg6 idle

\[bigdata-w-011\] gpadmin 202887 599486 0 15:30 ? 00:00:03 postgres:
port 40000, llys llysdb 10.223.6.8(55485) con83087 seg8 idle

\[bigdata-w-011\] gpadmin 202889 599486 0 15:30 ? 00:00:01 postgres:
port 40000, llys llysdb 10.223.6.8(55495) con83087 seg8 idle

\[bigdata-w-011\] gpadmin 202893 599486 0 15:30 ? 00:00:02 postgres:
port 40000, llys llysdb 10.223.6.8(55507) con83087 seg9 cmd10725 slice21
MPPEXEC SELECT

\[bigdata-w-011\] gpadmin 202896 599486 0 15:30 ? 00:00:01 postgres:
port 40000, llys llysdb 10.223.6.8(55513) con83087 seg6 idle

\[bigdata-w-011\] gpadmin 202901 599486 0 15:30 ? 00:00:04 postgres:
port 40000, llys llysdb 10.223.6.8(55515) con83087 seg7 cmd10725 slice5
MPPEXEC SELECT

\[bigdata-w-011\] gpadmin 202991 599486 0 15:30 ? 00:00:04 postgres:
port 40000, llys llysdb 10.223.6.8(55891) con83087 seg7 cmd10725 slice8
MPPEXEC SELECT

\[bigdata-w-011\] gpadmin 202995 599486 0 15:30 ? 00:00:04 postgres:
port 40000, llys llysdb 10.223.6.8(55911) con83087 seg7 cmd10725 slice1
MPPEXEC SELECT

> **其中特别注意** cmdid的匹配问题，如果QD端SQL运行是挂住的情况，QD的cmdid在进程列表的显示是小于QE进程中cmdid的，差值为1.
> 所以使用相同的连接号码比较合适有效，例如上例中的con83087。
>
> 上例中，通常最后的slice进程需要最后退出，所以大量的slice21进程残留，首先需要检查残留的小号码slice进程号的进程，如本例中slice1
> slice5
> 的是需要特别研究和作为入手点的，因为大号码slice号的进程是要消费小号码slice号进程的数据输出。并且这种残留可能发生在多个节点上，不一定只在一个节点上，所以需要到所有节点检查进程。
>
> **其中特别注意** 不同slice号码进程组直接不是简单的递增消费关系，即不是slice5只消费slice4的数据，不是slice21只消费slice20的数据，这个需要explain
> SQL之后对照检查slice间的依赖关系确认，如本例中，slice21事实上是直接消费slice1的数据的。

2.  **检查进程调用栈**

> 发现小号码slice号残留的QE进程后，使用gdb或者lldb可以挂载进目标进程，一般执行方式：
>
> gdb /usr/local/hawq/bin/postgres -p \<pid\>
>
> 进入后，首先检查线程信息和主要线程的调用栈：

-   检查线程列表的gdb命令为 info threads, lldb命令为 thread list。

-   选择某个线程作为检查对象的gdb命令为 thread , lldb 命令为 thread
    > select 。

-   确认了需要观察的线程后，执行bt命令，打开调用栈，gdb和lldb命令一样。如下例为调用栈的样子。

-   特别注意：退出gdb或者lldb挂载
    > 用命令quit，而不要简单的杀进程或者不退出，这样会导致后续进程运行的各种问题。

> 如果需要更详细gdb或者lldb命令
> 参照该链接 <https://lldb.llvm.org/lldb-gdb.html>

Thread 1 (Thread 0x7fcc9d38fdc0 (LWP 414876)):

\#0 0x00007fcc99704a3d in poll () from /lib64/libc.so.6

\#1 0x00000000008b0174 in pollAcks ()

\#2 0x00000000008af0cf in SendChunkUDP ()

\#3 0x00000000008a32ae in SendTupleChunkToAMS ()

\#4 0x00000000008a04b3 in SendTuple ()

\#5 0x0000000000655df3 in ExecMotion ()

\#6 0x0000000000633ce8 in ExecProcNode ()

\#7 0x000000000062e20f in ExecutePlan ()

\#8 0x000000000062ddee in ExecutorRun ()

\#9 0x000000000076505a in PortalRun ()

\#10 0x00000000007606f3 in PostgresMain ()

\#11 0x000000000071cac4 in ServerLoop ()

\#12 0x0000000000719a85 in PostmasterMain ()

\#13 0x000000000068d5a9 in main ()

> 对于postgres进程，我们几乎首先关注和发现问题都要从有PostmasterMain（）的线程看起，如上例。调用栈的概念就好比函数PostmasterMain调用ServerLoop(),
> ServerLoop()再调用PostgreMain(),
> 依此类推，知道\#0的位置表示当前暂停的位置。注意我们用gdb或者lldb挂载进去之后进程会自动暂停运行，我们检查调用栈等一系列操作都是出于进程暂停运行的状态。退出gdb或者lldb后，进程会继续正常运行。
>
> 回到本话题的检测，在客户使用老执行器interconnect时，我们发现进程在等待发送完成的话，一定可以从调用栈中发现：SendChunkUDP()
> 它在调用
> pollAcks()；如果进程在等待接受，一定可以欧诺个调用栈中发现receiveChunksUDP()。上例是典型的发送hang，下面给出接受hang的栈例子。

Thread 1 (Thread 0x7f470ea0edc0 (LWP 343797)):

\#0 0x00007f470b523cf2 in pthread\_cond\_timedwait@\@GLIBC\_2.3.2 ()
from /lib64/libpthread.so.0

\#1 0x00000000008afbdf in receiveChunksUDP ()

\#2 0x00000000008ae653 in RecvTupleChunkFromAnyUDP ()

\#3 0x00000000008a07c0 in RecvTupleFrom ()

\#4 0x0000000000655f6e in ExecMotion ()

\#5 0x0000000000633ce8 in ExecProcNode ()

\#6 0x000000000062e20f in ExecutePlan ()

\#7 0x000000000062ddee in ExecutorRun ()

\#8 0x000000000076505a in PortalRun ()

\#9 0x0000000000761c4d in exec\_simple\_query ()

\#10 0x000000000075e5be in PostgresMain ()

\#11 0x000000000071cac4 in ServerLoop ()

\#12 0x0000000000719a85 in PostmasterMain ()

\#13 0x000000000068d5a9 in main ()

3.  **导出interconnect的连接信息**

> 基本原理是用lldb或者gdb挂入然后调用一个内部已经写好的函数，把内部数据导出到指定的文本文件中。该函数名如下，因此我们调用它的时候需要提供pEntry和fname。pEntry是需要从调用栈中找出来，不同类型的slice找法略不同，
> fname是一个字符串指定输出的文件名，例如\"/home/gpadmin/dumpfile1\"。

void

dumpConnections(ChunkTransportStateEntry \*pEntry, const char \*fname);

> 导出连接信息的命令是：

1)  gdb /usr/local/hawq/bin/postgres -p

2)  bt 检查是否为发送/接受阻塞期待的调用栈，检查上节参考样例

3)  f 2 一般可以找到使用中的pEntry

4)  call dumpConnections(pEntry, \"\") 完成导出

5)  获取其它数据辅助解读文件

6)  quit

<!-- -->

4.  **识别一个slice进程是纯发送进程**

> 通过explain
> SQL可以得到执行的SQL会分割成多少个slice，处于最下层的slice则为纯发送的。如果在该slice编号的进程中看到发送等待的地方则应该找到SendChunkUDP，gdb中使用f
> 2找到SendChunkUDP后，其中应有一个变量pEntry保存目标发送连接的信息，pEntry中包含多个链接信息，为了获取当前实际使用哪个索引的链接，则分两个情况，取决于motion的类型
>
> 无论哪种MOTION，在导出文件中都会看到多个连接信息，具体连接数和QE数一致。一般对于处于发送阻塞状态进程，

-   BROADCAST MOTION， 则需要gdb中使用f 3， 然后p index 找到具体索引号

-   REDISTRIBUTION MOTION, 则需要gdb中使用f 4， 然后p
    > targetRoute找到具体索引号

> 一个发送连接导出信息可能如下，其中

-   第一行给出连接总数，本例中一共27个连接，因为有27个QE。

-   然后根据获得的连接索引号，去查看相应信息，例如如果得到的号码为1则应该检查
    > conns\[1\]。

-   从连接信息中可清楚获得连接对端进程号，对端地址，如下例中conns\[0\]中对端进程号为673540，对端地址10.223.6.1:33190

-   从连接信息中可获得本地发送缓冲区是否还有可用的空间，下例中conns\[0\]中capacity=4表示还有，如果capacity=0则表示没有，一般没有capacity是发送阻塞的表面原因。

-   查看消息基于序列号的发送和消费进展，如下例conns\[0\]中sentSeq=8
    > receivedAckSeq=8
    > consumedSeq=8表示已经发了8个，收到了8个的ack消息对端已经处理了8个。

-   sndQueue
    > List在不空的情况下表示没有发送到对端且没有被消费的消息，下例中conns\[1\]包含了4个发送成功，但还没被对端消费的包。

-   unackQueue List不空则表示有发过去包但没收到ack的情况。

    Entry connections: conn num 27

    ==================================

    conns\[0\] motNodeId=4: remoteContentId=0 pid=673540 sockfd=-1
    remote=10.223.6.1:33190 local= capacity=4 sentSeq=8 receivedAckSeq=8
    consumedSeq=8 rtt=7010 dev=8458
    deadlockCheckBeginTime=20108339420276 route=0 msgSize=428
    msgPos=(nil) recvBytes=0 tupleCount=1 waitEOS=0 stillActive=1
    stopRequested=0 state=6

    conn\_info \[DATA: seq 9 extraSeq 0\]: motNodeId 4, crc 0 len 8188
    srcContentId 26 dstDesContentId 0 srcPid 479615 dstPid 673540
    srcListenerPort 31505 dstListernerPort 33190 sendSliceIndex 4
    recvSliceIndex 21 sessionId 5199 icId 1 flags 0

    sndQueue List Length 0

    unackQueue List Length 0

    conns\[1\] motNodeId=4: remoteContentId=1 pid=673545 sockfd=-1
    remote=10.223.6.1:12331 local= capacity=0 sentSeq=4 receivedAckSeq=4
    consumedSeq=0 rtt=11819 dev=9348
    deadlockCheckBeginTime=20108339420048 route=1 msgSize=428
    msgPos=(nil) recvBytes=0 tupleCount=1 waitEOS=0 stillActive=1
    stopRequested=0 state=6

    conn\_info \[DATA: seq 9 extraSeq 0\]: motNodeId 4, crc 0 len 8188
    srcContentId 26 dstDesContentId 1 srcPid 479615 dstPid 673545
    srcListenerPort 31505 dstListernerPort 12331 sendSliceIndex 4
    recvSliceIndex 21 sessionId 5199 icId 1 flags 0

    sndQueue List Length 4

    Node 0, linkptr 0x352aca0 Packet Content \[DATA: seq 5 extraSeq 0\]:
    motNodeId 4, crc 0 len 8052 srcContentId 26 dstDesContentId 1 srcPid
    479615 dstPid 673545 srcListenerPort 31505 dstListernerPort 12331
    sendSliceIndex 4 recvSliceIndex 21 sessionId 5199 icId 1 flags 0

    Node 1, linkptr 0x3930c70 Packet Content \[DATA: seq 6 extraSeq 0\]:
    motNodeId 4, crc 0 len 8100 srcContentId 26 dstDesContentId 1 srcPid
    479615 dstPid 673545 srcListenerPort 31505 dstListernerPort 12331
    sendSliceIndex 4 recvSliceIndex 21 sessionId 5199 icId 1 flags 0

    Node 2, linkptr 0x3536f40 Packet Content \[DATA: seq 7 extraSeq 0\]:
    motNodeId 4, crc 0 len 8140 srcContentId 26 dstDesContentId 1 srcPid
    479615 dstPid 673545 srcListenerPort 31505 dstListernerPort 12331
    sendSliceIndex 4 recvSliceIndex 21 sessionId 5199 icId 1 flags 0

    Node 3, linkptr 0x354b3a0 Packet Content \[DATA: seq 8 extraSeq 0\]:
    motNodeId 4, crc 0 len 8188 srcContentId 26 dstDesContentId 1 srcPid
    479615 dstPid 673545 srcListenerPort 31505 dstListernerPort 12331
    sendSliceIndex 4 recvSliceIndex 21 sessionId 5199 icId 1 flags 0

5.  **识别一个slice进程是收发进程**

> 因为QE一定向QD汇总数据，所以QE一定是一个发送进程，是否有收数据能力，要看计划中是否有向下的儿子slice，有就一定是有收的需要。
>
> 对于检查接收连接的进程，调用dumpConnections前需要执行 p
> ic\_control\_info.isSender=0
> 触发导出接收数据。其它过程与上述方法没区别，一般接收数据是从连接中任意一个有数据就可以处理，所以不需要检查targetRoute等。
>
> 一个例子如下，其中

-   第一行给出连接总数，本例中一共27个连接，因为有27个QE。

-   从连接信息中可清楚获得连接对端进程号，对端地址，如下例中conns\[0\]中对端进程号为673564，检查ps可以找到对端进程在哪里

-   从连接信息中可获得本地接收缓冲区是否还有待消费的数据，下例中没有带消费数据，如果没有
    > 则接收数据的进程在阻塞是有理由的，如果有待消费的数据，则原理上不应该产生接收阻塞。

    Entry connections: conn num 27

    ==================================

    conns\[0\] motNodeId=13: remoteContentId=0 pid=673564 sockfd=-1
    remote= local= capacity=0 sentSeq=0 receivedAckSeq=0 consumedSeq=0
    rtt=0 dev=0 deadlockCheckBeginTime=0 route=0 msgSize=0
    msgPos=0x7fbbb02cff30 recvBytes=0 tupleCount=0 waitEOS=0
    stillActive=0 stopRequested=0 state=0

    conn\_info \[ACK: seq 2 extraSeq 1\]: motNodeId 13, crc 0 len 0
    srcContentId 0 dstDesContentId 14 srcPid 673564 dstPid 239741
    srcListenerPort 38422 dstListernerPort 26501 sendSliceIndex 13
    recvSliceIndex 21 sessionId 5199 icId 1 flags 9

    pkt\_q\_size=0 pkt\_q\_head=1 pkt\_q\_tail=1 pkt\_q=0x3217318

    conns\[1\] motNodeId=13: remoteContentId=0 pid=673569 sockfd=-1
    remote= local= capacity=0 sentSeq=0 receivedAckSeq=0 consumedSeq=0
    rtt=0 dev=0 deadlockCheckBeginTime=0 route=1 msgSize=0
    msgPos=0x7fbbb02dbf90 recvBytes=0 tupleCount=0 waitEOS=0
    stillActive=0 stopRequested=0 state=0

    conn\_info \[ACK: seq 2 extraSeq 1\]: motNodeId 13, crc 0 len 0
    srcContentId 1 dstDesContentId 14 srcPid 673569 dstPid 239741
    srcListenerPort 43855 dstListernerPort 26501 sendSliceIndex 13
    recvSliceIndex 21 sessionId 5199 icId 1 flags 9

    pkt\_q\_size=0 pkt\_q\_head=1 pkt\_q\_tail=1 pkt\_q=0x3217348

资源管理 {#资源管理 .35}
========

insufficient memory reserved for statement {#insufficient-memory-reserved-for-statement .36}
------------------------------------------

-   问题

> 执行查询时报错\"insufficient memory reserved for
> statement\",,\"Increase statement memory or reduce the number of
> Parquet tables to be scanned.\"

-   分析

1)  对于INSERT，每个未加压缩的表所占内存为rowgroupSize；而每个加压缩的表所占内存为2 \*
    rowgroupSize

2)  对于SELECT，每个未加压缩的表所占内存为Sum(toBeselectedAttributeLen)/wholeRecordLen \*
    rowgroupSize ；而每个加压缩的表所占内存为2 \*
    Sum(toBeselectedAttributeLen)/wholeRecordLen \* rowgroupSize

-   解答

1)  在session级别调大hawq\_rm\_stmt\_nvseg (默认值为0)：SET
    HAWQ\_RM\_STMT\_NVSEG=X; 或者

2)  在session级别调大hawq\_rm\_stmt\_vseg\_memory (默认值为128 MB)：SET
    HAWQ\_RM\_STMT\_VSEG\_MEMORY=\'Xmb';或者

3)  调大资源队列的资源量：ALTER RESOURCE QUEUE queue1 WITH
    (VSEG\_RESOURCE\_QUOTA=\'mem:Xmb');

-   参考

> https://community.pivotal.io/s/article/88-Getting-Insufficient-Memory-Reserved-for-Statement-Error-for-Parquet-Tables

too many open files {#too-many-open-files .36}
-------------------

-   问题

> 查询在执行时报错"too many open files"

-   分析

> 系统可用最大文件描述符设置太小

-   解答

> ulimit -n \$NUMBER\_OF\_MAX\_OPEN\_FILES
>
> echo \"fs.nr\_open = 10000000\" \>\> /etc/sysctl.conf
>
> echo \"fs.file-max = 11000000\" \>\> /etc/sysctl.conf

-   参考

> http://www.chengweiyang.cn/2015/11/14/how-to-enlarge-linux-open-files-upper-cell/

analyze偶尔占用太长时间 {#analyze偶尔占用太长时间 .36}
-----------------------

-   问题

> 一个2000万数据的表，在做了vacuum后，做analyze，前3次正常1秒钟就处理完了，但是第4次快2个小时还没analyze完成

-   分析

1)  正常处理很快，突然2小时未完成，表示遇到异常情况，查询pg\_stat\_activity检查是否有其他sql正在执行

2)  检查发现没有其他异常sql。但是analyze语句waiting\_resource为t，表示正在等待资源。通过select \*
    from pg\_resqueue\_status检查资源状态。

3)  检查发现pg\_default
    queue的rsqholders为20，已达到pg\_default的默认最大并发数。rsqwaiters为1，即analyze
    statment正在等待资源，所以耗时过久也未完成。

4)  资源使用未满，pg\_stat\_activity也未显示有其他sql，rsqholders为20
    未知。

-   解答

> 更改pg\_default的active
> stmts数或者停掉可控制的sql等待资源超时回收后再操作。

QE使用内存不到30%被oom kill {#qe使用内存不到30被oom-kill .36}
---------------------------

-   解答

> 设置vm.overcommit\_memory=2, 修改vm.overcommit\_ratio, 通过
> hawq\_rm\_memory\_limit\_perseg 限制内存使用

Hadoop相关 {#hadoop相关 .35}
==========

safe mode {#safe-mode .36}
---------

-   分析

> 当收到来自datanode的状态报告后，namenode根据配置，确定

1)  可用的block占总数的比例。

2)  可用的数据节点数量符合要求之后，离开安全模式。

-   解答

> 如果有必要，也可以通过命令强制离开安全模式：hadoop dfsadmin -safemode
> leave
>
> 与安全模式相关的主要配置在hdfs-site.xml中，主要有下面几个属性:

1)  dfs.namenode.replication.min: 最小的文件block副本数量，默认为1。

2)  dfs.namenode.safemode.threshold-pct:
    副本数达到最小要求的block占系统总block数的百分比，当实际比例超过该配置后，才能离开安全模式（但是还需要其他条件也满足）。默认为0.999f，也就是说符合最小副本数要求的block占比超过99.9%时，并且其他条件也满足才能离开安全模式。如果为小于等于0，则不会等待任何副本达到要求即可离开。如果大于1，则永远处于安全模式。

3)  dfs.namenode.safemode.min.datanodes:
    离开安全模式的最小可用datanode数量要求，默认为0.也就是即使所有datanode都不可用，仍然可以离开安全模式。

4)  dfs.namenode.safemode.extension:
    当集群可用block比例，可用datanode都达到要求之后，如果在extension配置的时间段之后依然能满足要求，此时集群才离开安全模式。单位为毫秒，默认为1.。这个配置主要是对集群的稳定程度做进一步的确认，避免达到要求后马上又不符合安全标准。

zookeeper listener port起不来 {#zookeeper-listener-port起不来 .36}
-----------------------------

-   解答

> zookeeper配置文件用的hostname，在/etc/hosts里hostname被resolve成127.0.0.1，需要让hostname能被正确resolve成内部ip地址

Requested data length longer than max configured RPC length {#requested-data-length-longer-than-max-configured-rpc-length .36}
-----------------------------------------------------------

-   问题

> NameNode停在safe mode并报错"Requested data length is longer than
> maximum configured RPC length"，导致HDFS集群无法写数据

-   从datanode给namenode的包含block信息的RPC包发送失败，原因是这些RPC包太大（比默认的设置64MB大）

-   解答

> 更改默认的RPC包大小，修改ipc.maximum.data.length 为134217728 (即128MB)

cannot fetch block locations {#cannot-fetch-block-locations .36}
----------------------------

-   问题

> 执行查询select count(\*) from partitioned\_table时报错：cannot fetch
> block locations: DETAIL: HdfsIoException: Unexpected:when unwrap the
> rpc remote exception
> \"java.lang.ArrayIndexOutBoundException\",java.lang.ArrayIndexOutOfBoundException
> in /hawq/depends/libhdfs3/src/server/NamenodeImpl.cpp: 77\'

-   分析

> 当HDFS datanode上出现currupt
> block上报给namenode时，namenode内存中blockmap和corruptReplicas中有几率信息不同步，这个客户端获取block信息时出现数组越界情况导致获取block
> location信息失败。Datenode上currupt block后续会被自动清理

-   解答

> 该问题是HDFS的bug（[[https://issues.apache.org/jira/browse/HDFS-11445]{.underline}](https://issues.apache.org/jira/browse/HDFS-11445)），在高版本hadoop（2.8.2,
> 2.9.0）中被修复。临时解决方案是TRUNCATE TABLE或者重建表。

无法访问zookeeper服务 {#无法访问zookeeper服务 .36}
---------------------

-   问题

> 访问zookeeper服务时报错：Error contacting service. It is probably not
> running.

-   分析

> 机器启动时默认打开了防火墙

-   解答

> systemctl stop firewalld

无法停止HDFS集群 {#无法停止hdfs集群 .36}
----------------

-   问题

> stop-dfs.sh不起作用，且显示：
>
> no namenode to stop
>
> no secondary namenode to stop
>
> no datanode to stop

-   解答

> hadoop\*.pid文件默认在/tmp目录，太长时间后它们被操作系统清理掉，导致停止HDFS时找不到对应的进程。直接用kill
> -9 \<PID\>即可

could not write in table {#could-not-write-in-table .36}
------------------------

-   问题

> 不能将数据插入表中，日志中出现Could not write in table: Input/Output
> error

-   解答

> hadoop dfsadmin -report出现DFS Used%:
> 99.96%，表示HDFS集群满了，先清理部分不需要的表，之后HDFS需要扩容。

time out waiting for a quorum of nodes to response {#time-out-waiting-for-a-quorum-of-nodes-to-response .36}
--------------------------------------------------

-   问题

> HDFS报错namenode down掉：Time out waitting 20000ms for a quorum of
> nodes to response

-   解答

> 调长HDFS配置文件的超时时长 dfs.qjournal.write-txns.timeout.ms

flush failed for required journal {#flush-failed-for-required-journal .36}
---------------------------------

-   问题

> HDFS namenode的journalnode经常服务中断：
>
> 2016-11-21 22:36:40,908 WARN
> org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Waited
> 19822 ms (timeout=20000 ms) for a response for sendEdits. No responses
> yet.
>
> 2016-11-21 22:36:41,088 FATAL
> org.apache.hadoop.hdfs.server.namenode.FSEditLog: Error: flush failed
> for required journal (JournalAndStream(mgr=QJM to
> \[192.168.58.183:8485, 192.168.58.181:8485, 192.168.58.182:8485\],
> stream=QuorumOutputStream starting at txid 24533))
>
> java.io.IOException: Timed out waiting 20000ms for a quorum of nodes
> to respond.
>
> at
> org.apache.hadoop.hdfs.qjournal.client.AsyncLoggerSet.waitForWriteQuorum(AsyncLoggerSet.java:137)
>
> at
> org.apache.hadoop.hdfs.qjournal.client.QuorumOutputStream.flushAndSync(QuorumOutputStream.java:107)
>
> at
> org.apache.hadoop.hdfs.server.namenode.EditLogOutputStream.flush(EditLogOutputStream.java:113)
>
> at
> org.apache.hadoop.hdfs.server.namenode.EditLogOutputStream.flush(EditLogOutputStream.java:107)
>
> at
> org.apache.hadoop.hdfs.server.namenode.JournalSet\$JournalSetOutputStream\$8.apply(JournalSet.java:533)
>
> at
> org.apache.hadoop.hdfs.server.namenode.JournalSet.mapJournalsAndReportErrors(JournalSet.java:393)
>
> at
> org.apache.hadoop.hdfs.server.namenode.JournalSet.access\$100(JournalSet.java:57)
>
> at
> org.apache.hadoop.hdfs.server.namenode.JournalSet\$JournalSetOutputStream.flush(JournalSet.java:529)
>
> at
> org.apache.hadoop.hdfs.server.namenode.FSEditLog.logSync(FSEditLog.java:639)
>
> at
> org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2645)
>
> at
> org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2520)
>
> at
> org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:579)
>
> at
> org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:394)
>
> at
> org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos\$ClientNamenodeProtocol\$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
>
> at
> org.apache.hadoop.ipc.ProtobufRpcEngine\$Server\$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
>
> at org.apache.hadoop.ipc.RPC\$Server.call(RPC.java:975)
>
> at org.apache.hadoop.ipc.Server\$Handler\$1.run(Server.java:2040)
>
> at org.apache.hadoop.ipc.Server\$Handler\$1.run(Server.java:2036)
>
> at java.security.AccessController.doPrivileged(Native Method)
>
> at javax.security.auth.Subject.doAs(Subject.java:422)
>
> at
> org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1656)
>
> at org.apache.hadoop.ipc.Server\$Handler.run(Server.java:2034)
>
> 2016-11-21 22:36:41,089 WARN
> org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Aborting
> QuorumOutputStream starting at txid 24533
>
> 2016-11-21 22:36:41,113 INFO org.apache.hadoop.util.ExitUtil: Exiting
> with status 1
>
> 2016-11-21 22:36:41,122 INFO org.apache.hadoop.ipc.Client: Retrying
> connect to server: Slave2/192.168.58.182:8485. Already tried 0
> time(s); maxRetries=45
>
> 2016-11-21 22:36:41,123 INFO org.apache.hadoop.ipc.Client: Retrying
> connect to server: Slave1/192.168.58.181:8485. Already tried 0
> time(s); maxRetries=45
>
> 2016-11-21 22:36:41,123 INFO org.apache.hadoop.ipc.Client: Retrying
> connect to server: StandByNameNode/192.168.58.183:8485. Already tried
> 0 time(s); maxRetries=45
>
> 2016-11-21 22:36:41,137 WARN
> org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Took
> 20050ms to send a batch of 1 edits (218 bytes) to remote journal
> 192.168.58.182:8485
>
> 2016-11-21 22:36:41,137 WARN
> org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Took
> 20052ms to send a batch of 1 edits (218 bytes) to remote journal
> 192.168.58.181:8485
>
> 2016-11-21 22:36:41,137 WARN
> org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Took
> 20065ms to send a batch of 1 edits (218 bytes) to remote journal
> 192.168.58.183:8485
>
> 2016-11-21 22:36:41,145 INFO
> org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN\_MSG:
>
> /\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*
>
> SHUTDOWN\_MSG: Shutting down NameNode at CentOSMaster/192.168.58.180
>
> \*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*/

-   解答

> 设置hdfs-site.xml中的超时时间如下：
>
> \<property\>
>
> \<name\>dfs.qjournal.start-segment.timeout.ms\</name\>
>
> \<value\>600000000\</value\>
>
> \</property\>
>
> \<property\>
>
> \<name\>dfs.qjournal.prepare-recovery.timeout.ms\</name\>
>
> \<value\>600000000\</value\>
>
> \</property\>
>
> \<property\>
>
> \<name\>dfs.qjournal.accept-recovery.timeout.ms\</name\>
>
> \<value\>600000000\</value\>
>
> \</property\>
>
> \<property\>
>
> \<name\>dfs.qjournal.prepare-recovery.timeout.ms\</name\>
>
> \<value\>600000000\</value\>
>
> \</property\>
>
> \<property\>
>
> \<name\>dfs.qjournal.accept-recovery.timeout.ms\</name\>
>
> \<value\>600000000\</value\>
>
> \</property\>
>
> \<property\>
>
> \<name\>dfs.qjournal.finalize-segment.timeout.ms\</name\>
>
> \<value\>600000000\</value\>
>
> \</property\>
>
> \<property\>
>
> \<name\>dfs.qjournal.select-input-streams.timeout.ms\</name\>
>
> \<value\>600000000\</value\>
>
> \</property\>
>
> \<property\>
>
> \<name\>dfs.qjournal.get-journal-state.timeout.ms\</name\>
>
> \<value\>600000000\</value\>
>
> \</property\>
>
> \<property\>
>
> \<name\>dfs.qjournal.new-epoch.timeout.ms\</name\>
>
> \<value\>600000000\</value\>
>
> \</property\>
>
> \<property\>
>
> \<name\>dfs.qjournal.write-txns.timeout.ms\</name\>
>
> \<value\>600000000\</value\>
>
> \</property\>

too many clients already {#too-many-clients-already .36}
------------------------

-   问题

> 执行查询时报错\"FATAL\",\"53300\",\"sorry, too many clients already\"

-   解答

> 说明连接已满，检查master与segment看是否超出连接限制；检查应用是否有连接溢出；删除不必要的连接或者使用连接池。

在访问hive外部表的时候报错如下：ERROR: the pxf protocol for external tables is deprecated，提示pxf协议已弃用  {#在访问hive外部表的时候报错如下error-the-pxf-protocol-for-external-tables-is-deprecated提示pxf协议已弃用 .36}
-------------------------------------------------------------------------------------------------------------

-   解答

> pxf已经在oushuDB内被disable了，请尝试用hdfs外表的方式访问hive里的文件。

在使用hive的外部表进行读取数据的时候显示 ERROR: text\_in: failed to get next tuple. File /hive/f00002\_ke\_e/000001\_0, offset is 0: missing data for last 17 column(s), line is xxxxxx {#在使用hive的外部表进行读取数据的时候显示-error-text_in-failed-to-get-next-tuple.-file-hivef00002_ke_e000001_0-offset-is-0-missing-data-for-last-17-columns-line-is-xxxxxx .36}
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

-   解答

> 出现了missing data for last 17
> column(s)，这张表有18列，这是分隔符不匹配导致的，把一行当作了一整列。在hive中，默认的分隔符为一个不可见字符，
> 在hawq的外表设置为(delimiter
> E\'\\x01\')即可，或者直接查看hive在hdfs上的文件，直接查看改行数据的分隔符是什么
>
> 先找到hive表对应的hdfs地址,然后参考下面的链接 <http://www.oushu.io/docs/ch/import-export.html#hdfs>
>
> 对于分区的hive表，可以用脚本写个循环

-   问题原因

> pxf是可以装的。
> 但是它的功能和OushuDB的可插拔存储外部表功能高度重合，且性能较差，目前OushuDB不准备兼容。

HDFS namenode全部挂掉，重启后oushudb查询个别表报错 {#hdfs-namenode全部挂掉重启后oushudb查询个别表报错 .36}
--------------------------------------------------

-   现象

    ![1](media/image1.png){width="5.752083333333333in"
    height="2.8472222222222223in"}

    ![2](media/image2.png){width="5.752777777777778in"
    height="1.2034722222222223in"}

-   分析

> HDFS bug, Fix Version/s: 2.9.0, 2.7.4, 3.0.0-alpha4, 2.8.2
>
> HDFS 出错 code

final int numCorruptNodes = countNodes(blk).corruptReplicas();

final int numCorruptReplicas = corruptReplicas.numCorruptReplicas(blk);

if (numCorruptNodes != numCorruptReplicas) {

LOG.warn(\"Inconsistent number of corrupt replicas for \"

\+ blk + \" blockMap has \" + numCorruptNodes

\+ \" but corrupt replicas map has \" + numCorruptReplicas);

}

final int numNodes = blocksMap.numNodes(blk);

final boolean isCorrupt = numCorruptReplicas == numNodes;

final int numMachines = isCorrupt ? numNodes: numNodes -
numCorruptReplicas;

DatanodeStorageInfo\[\] machines = new
DatanodeStorageInfo\[numMachines\]; // numMachines 为负，抛出异常！！！

> Datanode shutdown，由于 bug 的存在，导致 namenode 持有的两个关于 block
> 信息的数据结构不一致，接着根据这两个数据结构大小之差创建数组，因为大小之差为负，所以抛出
> NegativeArraySize 异常。
>
> jira 里面是这么说的：

Looks like the root cause for this inconsistency is:
BlockInfo\#setGenerationStampAndVerifyReplicas may remove a datanode
storage from the block\'s storage list, but still leave the storage in
the CorruptReplicasMap.

This inconsistency later can be fixed automatically, e.g., by a full
block report. But maybe we should consider using
BlockManager\#removeStoredBlock(BlockInfo, DatanodeDescriptor) to remove
all the records related to the block-dn pair.

> 更多详情见 jira：<https://issues.apache.org/jira/browse/HDFS-11445>\
> 提及同样问题的
> jira：<https://issues.apache.org/jira/browse/HDFS-12676>

-   解答

> 此类「can not fetch block」问题，通常使用 hdfs dfsadmin
> -triggerBlockReport 做一次 full report，就会自动恢复。
>
> 问题更加详细的信息可以查看issue：
>
> [[https://github.com/oushu-io/support/issues/411]{.underline}](https://github.com/oushu-io/support/issues/411)

安装部署 {#安装部署 .35}
========

hawq服务不能初始化或者启动 {#hawq服务不能初始化或者启动 .36}
--------------------------

-   分析

> 共享内存设置太小
>
> 磁盘空间被占满

-   解答

> sudo sysctl kern.sysv.shmmax=2147483648
>
> sudo sysctl kern.sysv.shmmin=1
>
> sudo sysctl kern.sysv.shmmni=64
>
> sudo sysctl kern.sysv.shmseg=16
>
> sudo sysctl kern.sysv.shmall=524288
>
> 清理磁盘空间

初始化时报错sync hawq-site.xml failed {#初始化时报错sync-hawq-site.xml-failed .36}
-------------------------------------

-   问题

> 初始化时报错hawq\_init:hdp3:gpadmin-\[ERROR\]:-sync hawq-site.xml
> failed

-   分析

> hawq-site.xml里多配了default\_hash\_table\_bucket\_number

-   解答

> 删除hawq-site.xml里的default\_hash\_table\_bucket\_number

hawq start失败报错 {#hawq-start失败报错 .36}
------------------

-   分析

> 查看是否有之前残留的postgres进程

-   解答

> kill之前残留的postgres进程，然后执行hawq start

hawq init standy的问题 {#hawq-init-standy的问题 .36}
----------------------

-   问题

> 在有两套OushuDB的集群上，给其中一套集群初始化standy节点，结果显示初始化成功，且gp\_segment\_configuration里显示standby节点正常，但gp\_master\_mirroring里显示standby是Not
> Configured

-   解答

> 分别将两个集群的master和standby部署在独立的机器上，然后初始化standby即可

hawq stop smart immediate fast的区别 {#hawq-stop-smart-immediate-fast的区别 .36}
------------------------------------

-   解答

> smart: 等没有数据库连接时停止数据库；
>
> immediate:
> 取消正在执行的查询并停止数据库，类似于pg\_cancel\_backend()然后stop
> smart
>
> fast:
> kill正在执行的事务并停止数据库，类似于pg\_terminate\_backend()然后stop
> smart

修改hawq的ssh端口 {#修改hawq的ssh端口 .36}
-----------------

-   解答

> 目前hawq不支持修改ssh的端口。

hawq init cluster时失败，提示输入密码 {#hawq-init-cluster时失败提示输入密码 .36}
-------------------------------------

-   解答

> 首先确保ssh
> localhost无密码，然后看看有无其他postgres的进程，需要先停掉。

hawq init cluster时失败，但已启成功magma service {#hawq-init-cluster时失败但已启成功magma-service .36}
------------------------------------------------

-   解答

> 赋予当前用户对\~/hawq-data-directory的写权限。sudo chmod -R 755
> \~/hawq-data-directory/masterdd \~/hawq-data-directory/segmentdd

VMWare related issue {#vmware-related-issue .36}
--------------------

-   解答

> Because there is Python package in OushuDB/HAWQ, in order to make it
> work with system installed Python, following steps are requied before
> initializing or upgrading OushuDB cluster.

hawq ssh -f hostfile

test -f /usr/local/hawq/bin/python && mv /usr/local/hawq/bin/python\*
/usr/local/hawq/share

test -d /usr/local/hawq/lib/python2.7 && mv
/usr/local/hawq/lib/python2.7 /usr/local/hawq/share

test -f /usr/local/hawq/lib/libpython2.7.so && mv
/usr/local/hawq/lib/libpython\* /usr/local/hawq/share

ETL {#etl .35}
===

gpfdist报警告Address already in use {#gpfdist报警告address-already-in-use .36}
-----------------------------------

-   解答

> gpfdist会轮询IPV4/IPV6地址，可以通过 gpfdist -b
> 127.0.0.1指定只监听IPV4地址

创建外部表报错create dir failed  {#创建外部表报错create-dir-failed .36}
--------------------------------

-   问题

> 遇到外表创建不了的问题， 显示报错是hdfs api create dir failed, errno:
> 13

-   解答

> 要创建建外部表的文件夹需要有权限

创建外部表报错cannot create table on HDFS {#创建外部表报错cannot-create-table-on-hdfs .36}
-----------------------------------------

-   问题

> 创建外部表失败。报错：cannot create table on HDFS when the service is
> not available.HINT: check HDFS service, hawq\_dfs\_url configuration
> and other hdfs filespaces

-   解答

> 要检查select \* from pg\_tablespace发现没有默认的tablespace
> dfs\_default。目前通过hawq\_init\_with\_hdfs来决定是否带着hdfs初始化，默认值是false，即默认的tablespace是magma，则初始化时不会创建默认tablespace:
> dfs\_default，所以创建外部表失败。如果需要外部表，则需要在hawq-site.xml中配置
> hawq\_init\_with\_hdfs 为true。

访问外部表时报错failed to get block location of path {#访问外部表时报错failed-to-get-block-location-of-path .36}
----------------------------------------------------

-   问题

> 访问外部表select \* from ext\_table时报错:pq:
> hdfsprotocol\_blocklocation: failed to get block location of path
> /tmp/hdfs//.csv. It is reported generally due to HDFS servie error or
> another session's ongoing writing. (exthdfs.c 151)

-   解答

> 建立外表时指定location要注意最后不带/ 。

使用gpfdist时报错Possible SYN flooding {#使用gpfdist时报错possible-syn-flooding .36}
--------------------------------------

-   问题

> 集群外新建的gpfdist服务器，但使用gpfdist时，报request\_sock\_TCP:Possible
> SYN flooding on port 9001.Sending cookies.check SNMP
> counters。查询文档说是TCP端口连接被占满，这个导致gpfdist服务一直挂掉。

-   分析

> 使用脚本查看TIME\_WAIT状态的连接数目watch -n 1 \"netstat -nt \| grep
> TIME\_WAIT \| wc -l\",
> 并检查系统参数net.core.somaxconn以及net.ipv4.tcp\_max\_syn\_backlog，发现都很小，而数据库集群上面跑的是50个并发，有2个master+8个segment。

-   解答

> 参照数据库集群，将系统参数net.ipv4.tcp\_max\_syn\_backlog由默认的1024改成200000，之后gpfdist正常跑起来。

如何通过JDBC将spark和hawq连接起来 {#如何通过jdbc将spark和hawq连接起来 .36}
---------------------------------

-   解答

> from pyspark import SparkContext
>
> from pyspark.sql import SQLContext
>
> sc = SparkContext()
>
> sqlContext = SQLContext(sc)
>
> df =
> sqlContext.read.format(\'jdbc\').options(url=\'jdbc:postgresql://node1:5432/postgres?user=myuser&password=password\',dbtable=\'test\').load()

data line too long {#data-line-too-long .36}
------------------

-   问题

> 查询外部表时报做：data line too long. likely due to invalid csv data
> ...

-   解答

> 检查是否有些数据行的长度超过了gp\_max\_csv\_line\_length（默认值为1048576
> Bytes）

数据迁移 {#数据迁移 .35}
========

从db2迁移时对于substr的处理 {#从db2迁移时对于substr的处理 .36}
---------------------------

-   问题

> 从db2函数脚本迁移到hawq脚本，对于substr，db2是按照字节数截取的，但是它的参数是按照一个中文字符占2个字节(GBK)设置的，但是hawq里面的中文占3个字节(UTF8)，想不重导数据迁移脚本解答

-   解答

> 查SELECT convert(substring(CONVERT(\'我比他说是\' ,\'utf8\', \'gbk\'),
> 3, 6) using
> gbk\_to\_utf8);但是pg8.2.15里的convert的输出不是bytea，需要些udf。具体如下：

1)  CREATE CAST (bytea AS text) WITHOUT FUNCTION;

2)  CREATE CAST (text AS bytea) WITHOUT FUNCTION;

3)  SELECT convert(substring(CONVERT(\'我比他说是\' ,\'utf8\',
    \'gbk\')::bytea, 1, 4)::text using gbk\_to\_utf8);

操作系统 {#操作系统 .35}
========

ulimit -a通过ssh不生效 {#ulimit--a通过ssh不生效 .36}
----------------------

-   解答

> 查看ssh的版本，ssh -V
>
> 如果版本是7.3及以下：修改/etc/ssh/sshd\_config中 UseLogin的值为yes
>
> 如果版本是7.4及以上，具体步骤如下：修改/etc/ssh/sshd\_config的UsePAM
> no为UsePAM yes，重启sshd服务service sshd restart即可。

linux关闭防火墙 {#linux关闭防火墙 .36}
---------------

-   解答

> systemctl stop iptables
>
> systemctl disable iptables
>
> systemctl stop firewalld
>
> systemctl disable firewalld

linux关闭selinux {#linux关闭selinux .36}
----------------

-   解答

> sed -i \\\"s/\^SELINUX\\=enforcing/SELINUX\\=disabled/g\\\"
> /etc/selinux/config
>
> setenforce 0

yum install时显示No module named site {#yum-install时显示no-module-named-site .36}
-------------------------------------

-   问题

> source完OushuDB环境变量greenplum\_path.sh
> 以后，执行yum安装等python相关操作时会报错:
>
> \[root\@server \~\]\# source /usr/local/hawq/greenplum\_path.sh
>
> \[root\@server \~\]\# yum install lrzsz
>
> Could not find platform independent libraries \<prefix\>
>
> Could not find platform dependent libraries \<exec\_prefix\>
>
> Consider setting \$PYTHONHOME to \<prefix\>\[:\<exec\_prefix\>\]
>
> ImportError: No module named site

-   解答

> 重开一个终端会话，其中不要source greenplum\_path.sh

linux上进行磁盘挂载 {#linux上进行磁盘挂载 .36}
-------------------

-   解答

> \[root\@server \~\]\# df -vhT \#查看现有挂载好的磁盘
>
> \[root\@server \~\]\# fdisk -l \#查看系统所有可以识别出的磁盘
>
> \[root\@server \~\]\# mkfs.xfs /dev/sdc
> \#初始化想要挂载的磁盘\[/dev/sdc\]
>
> \[root\@server \~\]\# fdisk -l \#查看初始化结果
>
> \[root\@server \~\]\# blkid \#查找准备挂载的磁盘的UUID
>
> \[root\@server \~\]\# mkdir /data1 \#为准备挂载的磁盘创建目录
>
> \[root\@server \~\]\# cp /etc/fstab /etc/fstab\_20200220
> \#在修改系统文件前备份原始文件
>
> \[root\@server \~\]\# vim /etc/fstab \#修改/etc/fstab文件举例
>
> UUID=345cddcd-3dae-4af2-8395-3edc59e2f53e /data1 xfs
> rw,noatime,inode64,allocsize=16m 0 0
>
> \[root\@server \~\]\# mount -a \# 读取/etc/fstab文件自动挂载磁盘
>
> \[root\@server \~\]\# df -vhT \#检查磁盘挂载结果

磁盘分卷 {#磁盘分卷 .36}
--------

-   解答

1.  查看现有挂载好的磁盘

    df -vhT

2.  查看系统所有可以识别出的磁盘

    fdisk -l

3.  生成创建卷组语句

    fdisk -l \| grep Disk \| grep 274 \| awk \'{print \$2}\' \| sed
    \'s/://g\' \| grep -v sdb \| xargs -i{} echo \" {} \" \| xargs \|
    xargs -I{} echo \"vgcreate /dev/vgdata {}\"

    注释：其中grep 274是指选取出磁盘大小为274的磁盘；grep -v
    sdb为剔除磁盘名为sdb的选项；vgcreate /dev/vgdata
    {}为生成命令vgcreate，同时生成默认卷组名/dev/vgdataX

4.  创建卷组

    vgcreate /dev/vgdata1 /dev/sdd /dev/sdf /dev/sdg /dev/sdh /dev/sdi
    /dev/sdj

    vgcreate /dev/vgdata2 /dev/sde /dev/sdk /dev/sdm /dev/sdl /dev/sdo
    /dev/sdn

    注释：vgcreate \[卷组名\] \[磁盘名\]

5.  划分物理卷

    lvcreate -l 100%VG -n vgdatalogical01 /dev/vgdata1

    lvcreate -l 100%VG -n vgdatalogical02 /dev/vgdata2

    注释：lvcreate -l 100%VG -n \[物理卷名\] \[卷组名\]

6.  查看操作结果

    lvdisplay

7.  格式化分区

    lvdisplay \| grep \"LV Path\" \| awk \'{print \"mkfs.xfs \"\$3}\'

    mkfs.xfs /dev/vgdata1/vgdatalogical01

    mkfs.xfs /dev/vgdata2/vgdatalogical02

8.  为新磁盘创建目录

    mkdir -p /{data1\_new,data2\_new}

    注释：mkdir -p /{\[目录名1\],\[目录名2\]}

9.  寻找新磁盘的UUID

    blkid

    或者

    blkid \| grep \"vgdatalogical\" \| awk \'{print \$2\" /dataX\_new
    xfs rw,noatime,inode64,allocsize=16m 0 0\"}\' \| sed \'s/\"//g\'

    UUID=5b672b8a-9d1f-47e8-a6c3-8ce8ae1e9436 /data1\_new xfs
    rw,noatime,inode64,allocsize=16m 0 0

    UUID=4545ac56-2e8b-4379-8989-9180a7751d50 /data2\_new xfs
    rw,noatime,inode64,allocsize=16m 0 0

10. 在修改系统文件前备份原始文件

    cp /etc/fstab /etc/fstab\_20200220

11. 修改/etc/fstab

    vim /etc/fstab

12. 通过读取/etc/fstab文件自动挂载磁盘

    mount -a

13. 检查磁盘挂载结果

    df -vhT

14. (Option) 磁盘内容复制转移

    cp -R /data1/\* /data1\_new/

    cp -R /data2/\* /data2\_new/

15. 卸载旧磁盘

    vim /etc/fstab

    reboot

16. (Option) 创建软链接

    cd /

    mv /data1 /tmp/

    mv /data2 /tmp/

    ln -s /data1\_new data1

    ln -s /data2\_new data2

如何检查操作系统版本和内核 {#如何检查操作系统版本和内核 .36}
--------------------------

-   解答

    cat /etc/redhat-release

    uname -a

如何更改主机名 {#如何更改主机名 .36}
--------------

-   解答

    编辑/etc/hostname文件中的主机名，需要使用root权限

    vim /etc/hostname

    hostname \[new hostname\]

Java相关 {#java相关 .36}
--------

-   解答

    检查Java版本

    java -version

    Java安装

    yum install -y java-1.8.0-openjdk java-1.8.0-openjdk-devel

    NOTE: So far, Java 1.8 or higher version is required.

    寻找Java的安装位置

    sudo find / -name \*jdk

    如何将Java软链接到指定位置

    mkdir \[specific directory\]

    ln -s \[java position\] \[specific directory\]

如何YUM源配置 {#如何yum源配置 .36}
-------------

-   解答

    将操作系统的iso上传到服务器，默认是安装到/目录下

    scp or sftp command

    创建新目录

    mkdir /\[folder name, example abc\]

    mount -o loop \[OS iso file name\]/\[folder name, abc\]

    配置新的本地repo文件

    vim /etc/yum.repos.d/\[xxx.repo\]

    Content in file:

    \[xxx\]

    name=xxx

    baseurl=file:///abc/

    gpgkey=file:///abc/\[GPG-KEY\]

    enabled=1

    gpgcheck=0

    检查yum.repos.d文件夹

    cd /etc/yum.repos.d

    ls

    启动新YUM源

    yum clean all

    yum list

    yum makecache

scp指令快速指导 {#scp指令快速指导 .36}
---------------

-   解答

    从远程服务器向本地复制文件：

    scp username\@from\_host:file.txt /local/directory/

    从本地向远程服务器复制文件：

    scp file.txt username\@to\_host:/remote/directory/

    从远程服务器向本地复制目录：

    scp -r username\@from\_host:/remote/directory/ /local/directory/

    从一台远程服务器向另一台远程服务器复制文件：

    scp username\@from\_host:/remote/directory/file.txt
    username\@to\_host:/remote/directory/

sftp指令（可以用于Linux和Windows系统间的文件传输） {#sftp指令可以用于linux和windows系统间的文件传输 .36}
--------------------------------------------------

-   解答

    使用以下指令连接目标服务器，并且输入用户名和密码

    sftp \[IP address\]

    从本地向服务器传输文件

    put \[local file path\] \[remote directory\]

    Example: put C:\\Users\\86151\\Downloads\\OushuDB\\data\_load.py
    /home/oushu/Oushu

    从远程服务器向本地传输文件

    get \[remote directory\] \[local file path\]

    Example: get /home/oushu/core.2428
    C:\\Users\\86151\\Downloads\\OushuDB

如何安装httpd {#如何安装httpd .36}
-------------

-   解答

    yum -y install httpd

    systemctl start httpd

    systemctl status httpd

客户端 {#客户端 .35}
======

pgAdminIII启动报错存储空间不足 {#pgadminiii启动报错存储空间不足 .36}
------------------------------

-   解答

    Windows在使用某个软件时，突然提示"存储空间不足，无法处理此命令"，通常是由于运行内存不足导致的，重启机器即可。

升级 {#升级 .35}
====

升级完成后部分任务运行报错，导致任务无法完成，出现connection pointer is NULL && bucket num与实际文件不符两个错误 {#升级完成后部分任务运行报错导致任务无法完成出现connection-pointer-is-null-bucket-num与实际文件不符两个错误 .36}
----------------------------------------------------------------------------------------------------------------

-   问题

    ![](media/image3.png){width="5.768055555555556in"
    height="2.0194444444444444in"}

-   分析&&解答

    对应第一次任务失败，主要两个报错信息，分别对应两个问题：

    1）、connection pointer is NULL

    分析：通过日志分析，定位这个问题是新版本一个已知问题，因为kdw正常情况上限qe(最小执行单位，进程)个数在5000，而系统参数net.core.somaxconn默认值为128。

    处理：将net.core.somaxconn调整为10000，该报错消失。

    2）、bucket num与实际文件不符

    分析：这个问题比较复杂，因为新版本对并行度的优化，我们将hawq\_rm\_nvseg\_perquery\_perseg\_limit参数的默认值从以前的6改为8,众邦环境没有设置这个值，所以升级后是8，本来这个点没什么影响，不过以为众邦用的是函数跑任务，作为hash表在函数里认的是hawq\_rm\_nvseg\_perquery\_perseg\_limit参数，所以导致以前表设置是24（4个计算节点，每节点6就是24）个文件，现在默认值是8(8\*4=32)，在任务导入时分成32个文件写入，最后文件是32但是hash定义是24，这对hash表来说就会有一个报错。

    处理：将问题的hash重建导数后，该报错消失

任务并发出现oom导致任务无法完成 {#任务并发出现oom导致任务无法完成 .36}
-------------------------------

-   问题

    ![](media/image4.jpeg){width="5.768055555555556in"
    height="3.2444444444444445in"}

-   分析&&解答

    分析：out of
    memory升级前没有这个报错，观察资源队列vsegresourcequota设置为2gb,这个设置比较大，事因为本身有占内存大的sql使用默认值会报错，不过一般特殊sql分配单独的资源队列，如果全部任务用一个资源队列计算下来一个session根据参数会最少一个节点启6个进程，每个进程2gb如果20个并发情况下最多内存=6\*2gb\*20=240gb,已经非常大了。

    因为之前没有报这个错，考虑是升级前调整了seg\_max\_connections为5000，增加了节点进程数。

    处理：将seg\_max\_connections回退为以前的3000，没有出现oom报错。

有部分任务随机hang住 {#有部分任务随机hang住 .36}
--------------------

-   问题

    ![](media/image5.jpeg){width="5.768055555555556in"
    height="3.2444444444444445in"}

-   分析&&解答

    分析：并发大的时候会有个别任务hang住，直到3600秒报错，通过日志、栈分析发现是gp\_vmem\_idle\_resource\_timeout参数设置不正确导致的，因为之前处理其他问题时，改过这个参数测试，改回来时设置参数错误导致，之前这个参数默认值是18s后来改回为18ms,因为系统本身在任务并发大的时候有丢包现象，丢包加上这个参数设置过小会出现节点间任务退出不一致，导致节点间进程收发失败。

    处理：将gp\_vmem\_idle\_resource\_timeout参数修改为18s，没有出现hang住现象。

如何修改系统表找回丢失的列信息 {#如何修改系统表找回丢失的列信息 .36}
------------------------------

-   分析&&解答

    1.查询丢失列的用户表的oid, 给第二步使用\
    select oid, \* from pg\_class where relname=\*\*\*;

    2.确认丢失列是否在pg\_attribute系统表中存在，
    如果能够查到数据，说明这种情况可以恢复删除的列；\
    select attrelid, attname, attnum, atttypid from pg\_attribute where
    attisdropped=\'t\' and attrelid=\*\*\*;\
    这种情况下，通常attname不是真实的列名， atttypid通常为0

    3.找到丢失列的真实名称， 以及
    丢失之前的真实类型oid(在pg\_type这个表中根据typname查找oid)

    4.上述步骤都完成后，开启进行元数据的修改，\
    show allow\_system\_table\_mods; （默认为NONE) set
    allow\_system\_table\_mods = \'DML\';

    5.通过修改pg\_attribute将丢失的列恢复\
    update pg\_attribute set attname=original\_name,
    atttypid=original\_id attisdropped=\'f\' where attrelid=\*\*\* and
    attnum=original\_col\_num and attisdropped=\'t\';

    6.查询原先丢失的列是否已经恢复\
    6.1 查询表结构： \\d table\_name\
    6.2 查询表数据： select \* from table\_name limit 10;

    7.关闭元数据修改模式：set allow\_system\_table\_mods=\'none\';

    8.根据需要最好重新做一下 analyze table\_name;

如何在hawq升级过程中安装postgis {#如何在hawq升级过程中安装postgis .36}
-------------------------------

-   分析&&解答

    根因已经找到，升级过程中执行了postgis的uninstall脚本，导致对postgis中udt存在依赖的表出现列删除情况；\
    经过测试实验后，总结后续,在已经安装了oushu-postgis的情况下，hawq在升级时按照如下方式安装oushu-postgis（在oushu-postgis本身没有升级的情况下）：\
    sudo yum remove -y hawq\
    sudo yum remove -y oushu-geos oushu-proj\
    sudo yum install -y hawq\
    upgrade （copy masterdd, etc configuration files）\
    sudo yum install -y oushu-postgis
